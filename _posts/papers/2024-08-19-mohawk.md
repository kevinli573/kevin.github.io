---
layout: paper
categories: papers
permalink: papers/mohawk
id: mohawk
title: "Transformers to SSMs: Distilling Quadratic Knowledge to Subquadratic Models"
authors: 
  - Aviv Bick*
  - Kevin Y. Li*
  - Eric P. Xing
  - J. Zico Kolter
  - Albert Gu
venue: "Conference on Neural Information Processing Systems"
venue-shorthand: NeurIPS
location: Vancouver, Canada
year: 2024
url: /papers/mohawk
pdf: https://arxiv.org/abs/2408.10189
# poster: /assets/argo_scholar_poster.pdf
code: https://github.com/goombalab/phi-mamba
# award: Best Poster, Honorable Mention
type: conference
# figure: /images/papers/21-argo-scholar.png
# doi: "10.48550/arXiv.2110.14060"

# highlight:
selected: true
featured: true
feature-order: 0
feature-title: 'Cross-Architectural Distillation'
feature-description: Cross-architectural distillation from Transformers to Mamba models with just a fraction of the pretraining data.
# image: /images/papers/coming-soon.png
# coming-soon: true

bibtex: |-

  @misc{bick2024transformersssmsdistillingquadratic,
      title={Transformers to SSMs: Distilling Quadratic Knowledge to Subquadratic Models}, 
      author={Aviv Bick and Kevin Y. Li and Eric P. Xing and J. Zico Kolter and Albert Gu},
      year={2024},
      eprint={2408.10189},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2408.10189}, 
  }
---

Transformer architectures have become a dominant paradigm for domains like language modeling but suffer in many inference settings due to their quadratic-time self-attention. Recently proposed subquadratic architectures, such as Mamba, have shown promise, but have been pretrained with substantially less computational resources than the strongest Transformer models. In this work, we present a method that is able to distill a pretrained Transformer architecture into alternative architectures such as state space models (SSMs). The key idea to our approach is that we can view both Transformers and SSMs as applying different forms of mixing matrices over the token sequences. We can thus progressively distill the Transformer architecture by matching different degrees of granularity in the SSM: first matching the mixing matrices themselves, then the hidden units at each block, and finally the end-to-end predictions. Our method, called MOHAWK, is able to distill a Mamba-2 variant based on the Phi-1.5 architecture (Phi-Mamba) using only 3B tokens and a hybrid version (Hybrid Phi-Mamba) using 5B tokens. Despite using less than 1% of the training data typically used to train models from scratch, Phi-Mamba boasts substantially stronger performance compared to all past open-source non-Transformer models. MOHAWK allows models like SSMs to leverage computational resources invested in training Transformer-based architectures, highlighting a new avenue for building such models.